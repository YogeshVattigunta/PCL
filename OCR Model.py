# -*- coding: utf-8 -*-
"""ASEAS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bLnb2DcAgzALT_VrfDQ5mM6JV550l_hr
"""

import cv2

def preprocess_handwriting(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    img = cv2.GaussianBlur(img, (5, 5), 0)  # Reduce noise
    _, binary_img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return binary_img
print(preprocess_handwriting('/content/img.jpg'))

!pip install easyocr

!pip install opencv-python-headless

import easyocr
import cv2
import matplotlib.pyplot as plt

def recognize_text_easyocr(image_path):
    # Initialize EasyOCR reader (specify languages; default is English)
    reader = easyocr.Reader(['en'])

    # Read the image and process
    results = reader.readtext(image_path)

    # Display the detected text and bounding boxes
    for bbox, text, confidence in results:
        print(f"Detected text: '{text}' with confidence: {confidence}")

    return results


image_path = "/content/img.jpg"
recognized_text = recognize_text_easyocr(image_path)

def preprocess_image(image_path):
    # Load the image in grayscale
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Apply GaussianBlur to remove noise
    img = cv2.GaussianBlur(img, (5, 5), 0)

    # Apply adaptive thresholding for binarization
    processed_img = cv2.adaptiveThreshold(
        img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
    )

    return processed_img

# Save preprocessed image to visualize it
image_path = "/content/img.jpg"
processed_img = preprocess_image(image_path)
cv2.imwrite("processed_image.jpg", processed_img)

import cv2

# Define Preprocessing Function
def preprocess_image_advanced(image_path, output_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    # Denoise and improve contrast
    img = cv2.GaussianBlur(img, (5, 5), 0)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    img = clahe.apply(img)
    # Adaptive thresholding
    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
    cv2.imwrite(output_path, img)
    return output_path

# Process the input image
input_image_path = "/content/img.jpg"
processed_image_path = "/content/img.jpg"
processed_image_path = preprocess_image_advanced(input_image_path, processed_image_path)

import easyocr

# Initialize EasyOCR Reader
reader = easyocr.Reader(['en'], gpu=False)

# Perform OCR on the processed image
results = reader.readtext(processed_image_path, detail=1, paragraph=False, contrast_ths=0.7)

# Print results for debugging
for bbox, text, confidence in results:
    print(f"Text: {text}, Confidence: {confidence}")

!pip install pyspellchecker

!pip show pyspellchecker

from spellchecker import SpellChecker

   # Initialize the SpellChecker

spell = SpellChecker()

def correct_text(results):
       corrected = []
       for bbox, text, confidence in results:
           corrected_text = spell.correction(text)  # Correct the text
           corrected.append((bbox, corrected_text, confidence))
       return corrected

   # Correct OCR results
corrected_results = correct_text(results)

   # Print corrected results
for bbox, text, confidence in corrected_results:
  print(f"Corrected Text: {text}, Confidence: {confidence}")

#Drawing boxes over detected text
import matplotlib.pyplot as plt
def draw_boxes(image_path, results):
    # Load the original image
    img = cv2.imread(image_path)

    # Loop through detected text and draw bounding boxes
    for bbox, text, confidence in results:
        # Get coordinates of the bounding box
        top_left = tuple(map(int, bbox[0]))
        bottom_right = tuple(map(int, bbox[2]))

        # Draw the rectangle
        cv2.rectangle(img, top_left, bottom_right, (0, 255, 0), 2)

        # Put the detected text on the image
        cv2.putText(
            img, text, (top_left[0], top_left[1] - 10),
            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2
        )

    # Display the image
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.axis("off")
    plt.show()

# Draw bounding boxes on the original image
draw_boxes('/content/processed_image.jpg', recognized_text)

#Saving detected text
def save_text_to_file(results, output_file="recognized_text.txt"):
    with open(output_file, "w") as file:
        for bbox, text, confidence in results:
            file.write(f"Text: {text}\nConfidence: {confidence:.2f}\n\n")

# Save recognized text to a file
save_text_to_file(recognized_text)

#NLP for Tagging
def segment_answers(results):
    answers = {}
    current_question = None
    current_answer = []

    for bbox, text, confidence in results:
        if text.strip().isdigit():  # Question number
            if current_question:
                answers[current_question] = " ".join(current_answer)
            current_question = text.strip()
            current_answer = []
        else:
            current_answer.append(text.strip())

    if current_question:
        answers[current_question] = " ".join(current_answer)

    return answers

segmented_answers = segment_answers(recognized_text)
print(segmented_answers)

import google.generativeai as genai
import PIL.Image
import os  # Import os to check file existence

# Configure API Key
genai.configure(api_key="AIzaSyA0npdKO-sEMEzENqi9s-VN5wXftMsX7rk")

# Load Gemini Model
model = genai.GenerativeModel("gemini-1.5-pro")

# Define Image Path (Use raw string format)
image_path = r"/content/img.jpg"

# Function to Load Image Safely
def load_image(image_path):
    if not os.path.exists(image_path):
        print(f"❌ Error: Image file not found at {image_path}")
        return None
    try:
        return PIL.Image.open(image_path)
    except Exception as e:
        print(f"❌ Error opening image: {e}")
        return None

# Load the image
organ = load_image(image_path)

# Proceed if image is successfully loaded
if organ:
    response = model.generate_content([organ, "Analyze the given image and carefully extract the information."])
    print(response.text)
else:
    print("Image could not be loaded.")

from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image
import torch

# Load the TrOCR processor and model from Hugging Face
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-large-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-large-handwritten")

# Open the image of handwritten text
image_path = "/content/img.jpg"
image = Image.open(image_path)

# Preprocess the image for TrOCR
inputs = processor(images=image, return_tensors="pt")  # Returns 'pixel_values'

# Perform inference (text generation)
with torch.no_grad():  # Turn off gradients for inference
    pixel_values = inputs.pixel_values  # Correct key
    generated_ids = model.generate(pixel_values=pixel_values)

# Decode the generated text
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

# Print the recognized text
print("Recognized Handwritten Text:")
print(generated_text)

"""**USING BERT TO CORRECT THE EXTRACTED TEXT**"""

import requests
import torch
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score

# Hugging Face API Key (Replace with your own key)
API_KEY = "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"

# Load BERT-based sentence similarity model
similarity_model = SentenceTransformer("all-mpnet-base-v2")

# Load BERTScore model
bert_model = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model)

# Load Question Answering Model
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Function to get similarity score
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    cosine_score = util.pytorch_cos_sim(embeddings1, embeddings2).item()
    return cosine_score

# Function to get BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information using QA Model
def extract_info(text, question):
    return qa_pipeline(question=question, context=text)['answer']

# Example usage
student_answer = "Pollution is harmful to human health. It causes respiratory diseases and environmental damage."
key_answer = "Air pollution leads to serious health issues like asthma and affects the ecosystem."

similarity = get_similarity_score(student_answer, key_answer)
bert_similarity = get_bert_score(student_answer, key_answer)
extracted_info = extract_info(student_answer, "What are the effects of pollution?")

# Weighted scoring
final_score = (similarity * 0.5) + (bert_similarity * 0.5)

print(f"Cosine Similarity Score: {similarity * 100:.2f}%")
print(f"BERTScore Similarity: {bert_similarity * 100:.2f}%")
print(f"Extracted Information: {extracted_info}")
print(f"Final NLP-based Score: {final_score * 100:.2f}%")

pip install pytesseract

import cv2
import pytesseract
import torch
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score

# Load models
similarity_model = SentenceTransformer("all-mpnet-base-v2")
bert_model_name = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Set up Tesseract OCR path (change this based on your OS)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Windows path

# Function to extract text from an image using OCR
def extract_text_from_image(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
    text = pytesseract.image_to_string(gray)  # Extract text
    return text.strip()

# Function to compute Cosine Similarity
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    return util.pytorch_cos_sim(embeddings1, embeddings2).item()

# Function to compute BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information
def extract_info(text, question="What are the key points?"):
    return qa_pipeline(question=question, context=text)['answer']

# Function to read text from a file
def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read().strip()

# Function to analyze an answer sheet image
def analyze_answer_sheet(image_path, key_file, output_file="result.txt"):
    # Extract text from the image
    extracted_text = extract_text_from_image(image_path)

    # Read the key answer
    key_answer = read_text_file(key_file)

    # Calculate similarity scores
    cosine_similarity = get_similarity_score(extracted_text, key_answer)
    bert_similarity = get_bert_score(extracted_text, key_answer)
    extracted_info = extract_info(extracted_text)

    # Weighted final score
    final_score = (cosine_similarity * 0.5) + (bert_similarity * 0.5)

    # Save results to a text file
    with open(output_file, "w", encoding="utf-8") as file:
        file.write(f"Extracted Answer Text: {extracted_text}\n\n")
        file.write(f"Cosine Similarity Score: {cosine_similarity * 100:.2f}%\n")
        file.write(f"BERTScore Similarity: {bert_similarity * 100:.2f}%\n")
        file.write(f"Extracted Key Information: {extracted_info}\n")
        file.write(f"Final NLP-based Score: {final_score * 100:.2f}%\n")

    print(f"Results saved in {output_file}")

# Example usage (Replace with actual file paths)
analyze_answer_sheet("/content/img.jpg", "key_answer.txt")

import cv2
import pytesseract
import torch
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score

# Load models
similarity_model = SentenceTransformer("all-mpnet-base-v2")
bert_model_name = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Set up Tesseract OCR path (change this based on your OS)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Windows path

# Function to extract text from an image using OCR
def extract_text_from_image(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
    text = pytesseract.image_to_string(gray)  # Extract text
    return text.strip()

# Function to compute Cosine Similarity
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    return util.pytorch_cos_sim(embeddings1, embeddings2).item()

# Function to compute BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information
def extract_info(text, question="What are the key points?"):
    return qa_pipeline(question=question, context=text)['answer']

# Function to read text from a file
def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read().strip()

# Function to analyze an answer sheet image
def analyze_answer_sheet(image_path, key_file, output_file="result.txt"):
    # Extract text from the image
    extracted_text = extract_text_from_image(image_path)

    # Read the key answer
    key_answer = read_text_file(key_file)

    # Calculate similarity scores
    cosine_similarity = get_similarity_score(extracted_text, key_answer)
    bert_similarity = get_bert_score(extracted_text, key_answer)
    extracted_info = extract_info(extracted_text)

    # Weighted final score
    final_score = (cosine_similarity * 0.5) + (bert_similarity * 0.5)

    # Save results to a text file
    with open(output_file, "w", encoding="utf-8") as file:
        file.write(f"Extracted Answer Text: {extracted_text}\n\n")
        file.write(f"Cosine Similarity Score: {cosine_similarity * 100:.2f}%\n")
        file.write(f"BERTScore Similarity: {bert_similarity * 100:.2f}%\n")
        file.write(f"Extracted Key Information: {extracted_info}\n")
        file.write(f"Final NLP-based Score: {final_score * 100:.2f}%\n")

    print(f"Results saved in {output_file}")

# Example usage (Replace with actual file paths)
analyze_answer_sheet("/content/img.jpg", "key_answer.txt")

import cv2
import pytesseract
import torch
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score

# Load models
similarity_model = SentenceTransformer("all-mpnet-base-v2")
bert_model_name = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Set up Tesseract OCR path (change this based on your OS)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Windows path

# Function to extract text from an image using OCR
def extract_text_from_image(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
    text = pytesseract.image_to_string(gray)  # Extract text
    return text.strip()

# Function to compute Cosine Similarity
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    return util.pytorch_cos_sim(embeddings1, embeddings2).item()

# Function to compute BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information
def extract_info(text, question="What are the key points?"):
    return qa_pipeline(question=question, context=text)['answer']

# Function to read text from a file
def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read().strip()

# Function to analyze an answer sheet image
def analyze_answer_sheet(image_path, key_file, output_file="result.txt"):
    # Extract text from the image
    extracted_text = extract_text_from_image(image_path)

    # Read the key answer
    key_answer = read_text_file(key_file)

    # Calculate similarity scores
    cosine_similarity = get_similarity_score(extracted_text, key_answer)
    bert_similarity = get_bert_score(extracted_text, key_answer)
    extracted_info = extract_info(extracted_text)

    # Weighted final score
    final_score = (cosine_similarity * 0.5) + (bert_similarity * 0.5)

    # Save results to a text file
    with open(output_file, "w", encoding="utf-8") as file:
        file.write(f"Extracted Answer Text: {extracted_text}\n\n")
        file.write(f"Cosine Similarity Score: {cosine_similarity * 100:.2f}%\n")
        file.write(f"BERTScore Similarity: {bert_similarity * 100:.2f}%\n")
        file.write(f"Extracted Key Information: {extracted_info}\n")
        file.write(f"Final NLP-based Score: {final_score * 100:.2f}%\n")

    print(f"Results saved in {output_file}")

# Example usage (Replace with actual file paths)
analyze_answer_sheet("/content/img.jpg", "C:\Users\Admin\Downloads\key_answer.txt")

import cv2
import pytesseract
import torch
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score

# Load models
similarity_model = SentenceTransformer("all-mpnet-base-v2")
bert_model_name = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Set up Tesseract OCR path (change this based on your OS)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Windows path

# Function to extract text from an image using OCR
def extract_text_from_image(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
    text = pytesseract.image_to_string(gray)  # Extract text
    return text.strip()

# Function to compute Cosine Similarity
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    return util.pytorch_cos_sim(embeddings1, embeddings2).item()

# Function to compute BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information
def extract_info(text, question="What are the key points?"):
    return qa_pipeline(question=question, context=text)['answer']

# Function to read text from a file
def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read().strip()

# Function to analyze an answer sheet image
def analyze_answer_sheet(image_path, key_file, output_file="result.txt"):
    # Extract text from the image
    extracted_text = extract_text_from_image(image_path)

    # Read the key answer
    key_answer = read_text_file(key_file)

    # Calculate similarity scores
    cosine_similarity = get_similarity_score(extracted_text, key_answer)
    bert_similarity = get_bert_score(extracted_text, key_answer)
    extracted_info = extract_info(extracted_text)

    # Weighted final score
    final_score = (cosine_similarity * 0.5) + (bert_similarity * 0.5)

    # Save results to a text file
    with open(output_file, "w", encoding="utf-8") as file:
        file.write(f"Extracted Answer Text: {extracted_text}\n\n")
        file.write(f"Cosine Similarity Score: {cosine_similarity * 100:.2f}%\n")
        file.write(f"BERTScore Similarity: {bert_similarity * 100:.2f}%\n")
        file.write(f"Extracted Key Information: {extracted_info}\n")
        file.write(f"Final NLP-based Score: {final_score * 100:.2f}%\n")

    print(f"Results saved in {output_file}")

# Example usage (Replace with actual file paths)
analyze_answer_sheet("/content/img.jpg", "key_answer.txt")

import cv2
import pytesseract
import torch
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score

# Load models
similarity_model = SentenceTransformer("all-mpnet-base-v2")
bert_model_name = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Set up Tesseract OCR path (change this based on your OS)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"  # Windows path

# Function to extract text from an image using OCR
def extract_text_from_image(image_path):
    img = cv2.imread(image_path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
    text = pytesseract.image_to_string(gray)  # Extract text
    return text.strip()

# Function to compute Cosine Similarity
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    return util.pytorch_cos_sim(embeddings1, embeddings2).item()

# Function to compute BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information
def extract_info(text, question="What are the key points?"):
    return qa_pipeline(question=question, context=text)['answer']

# Function to read text from a file
def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read().strip()

# Function to analyze an answer sheet image
def analyze_answer_sheet(image_path, key_file, output_file="result.txt"):
    # Extract text from the image
    extracted_text = extract_text_from_image(image_path)

    # Read the key answer
    key_answer = read_text_file(key_file)

    # Calculate similarity scores
    cosine_similarity = get_similarity_score(extracted_text, key_answer)
    bert_similarity = get_bert_score(extracted_text, key_answer)
    extracted_info = extract_info(extracted_text)

    # Weighted final score
    final_score = (cosine_similarity * 0.5) + (bert_similarity * 0.5)

    # Save results to a text file
    with open(output_file, "w", encoding="utf-8") as file:
        file.write(f"Extracted Answer Text: {extracted_text}\n\n")
        file.write(f"Cosine Similarity Score: {cosine_similarity * 100:.2f}%\n")
        file.write(f"BERTScore Similarity: {bert_similarity * 100:.2f}%\n")
        file.write(f"Extracted Key Information: {extracted_info}\n")
        file.write(f"Final NLP-based Score: {final_score * 100:.2f}%\n")

    print(f"Results saved in {output_file}")

# Example usage (Replace with actual file paths)
analyze_answer_sheet("/content/img.jpg", "key_answer.txt")

from google.colab import files
uploaded = files.upload()

!pip install easyocr
!pip install bert-score
!pip install sentence-transformers transformers
!pip install transformers requests

import cv2
import easyocr
import torch
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score
import re

# Load models for similarity analysis
similarity_model = SentenceTransformer("all-mpnet-base-v2")
bert_model_name = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Function to clean and preprocess extracted text
def preprocess_text(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Function to extract text from an image using EasyOCR
def extract_text_from_image(image_path):
    reader = easyocr.Reader(['en'])
    results = reader.readtext(image_path)
    extracted_text = " ".join([text for bbox, text, confidence in results])
    return preprocess_text(extracted_text)  # Preprocess the extracted text

# Function to compute Cosine Similarity
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    return util.pytorch_cos_sim(embeddings1, embeddings2).item()

# Function to compute BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information using question answering pipeline
def extract_info(text, question="What are the key points?"):
    return qa_pipeline(question=question, context=text)['answer']

# Function to read text from a file
def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read().strip()

# Function to analyze an answer sheet image
def analyze_answer_sheet(image_path, key_file, output_file="result.txt"):
    # 1. Extract text from the image using EasyOCR
    extracted_text = extract_text_from_image(image_path)

    # 2. Read the key answer from the file
    key_answer = read_text_file(key_file)

    # 3. Perform Similarity Analysis
    cosine_similarity = get_similarity_score(extracted_text, key_answer)
    bert_similarity = get_bert_score(extracted_text, key_answer)
    extracted_info = extract_info(extracted_text)

    # 4. Calculate Weighted Score
    final_score = (cosine_similarity * 0.7) + (bert_similarity * 0.3)  # Increase weight of cosine similarity

    # 5. Save results to a text file
    with open(output_file, "w", encoding="utf-8") as file:
        file.write(f"Extracted Answer Text: {extracted_text}\n\n")
        file.write(f"Cosine Similarity Score: {cosine_similarity * 100:.2f}%\n")
        file.write(f"BERTScore Similarity: {bert_similarity * 100:.2f}%\n")
        file.write(f"Extracted Key Information: {extracted_info}\n")
        file.write(f"Final NLP-based Score: {final_score * 100:.2f}%\n")

    print(f"Results saved in {output_file}")

# Example usage
analyze_answer_sheet("/content/img.jpg", "/content/key_answer.txt")  # Upload first

import nltk
nltk.download('punkt')
nltk.download('stopwords')

import nltk
nltk.download()

from nltk.corpus import brown
nltk.download('brown')

nltk.set_proxy('http://proxy.example.com:3128', ('USERNAME', 'PASSWORD'))
nltk.download()

import cv2
import easyocr
import torch
import re
import nltk
import requests
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spellchecker import SpellChecker
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Load models for similarity analysis
similarity_model = SentenceTransformer("all-mpnet-base-v2")
bert_model_name = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Initialize SpellChecker
spell = SpellChecker()

# Function to clean and preprocess extracted text
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]
    corrected_text = [spell.correction(word) if spell.correction(word) else word for word in filtered_text]
    return " ".join(corrected_text)

# Function to extract text from an image using EasyOCR
def extract_text_from_image(image_path):
    reader = easyocr.Reader(['en'])
    results = reader.readtext(image_path)
    extracted_text = " ".join([text for _, text, _ in results])
    return preprocess_text(extracted_text)

# Function to compute Cosine Similarity
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    return util.pytorch_cos_sim(embeddings1, embeddings2).item()

# Function to compute BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information using a question-answering model
def extract_info(text, question="What are the key points?"):
    return qa_pipeline(question=question, context=text)['answer']

# Function to read text from a file
def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read().strip()

# Function to analyze an answer sheet image
def analyze_answer_sheet(image_path, key_file, output_file="result.txt"):
    try:
        extracted_text = extract_text_from_image(image_path)
        key_answer = read_text_file(key_file)
        cosine_similarity = get_similarity_score(extracted_text, key_answer)
        bert_similarity = get_bert_score(extracted_text, key_answer)
        extracted_info = extract_info(extracted_text)
        final_score = (cosine_similarity * 0.75) + (bert_similarity * 0.25)

        with open(output_file, "w", encoding="utf-8") as file:
            file.write(f"Extracted Answer Text: {extracted_text}\n\n")
            file.write(f"Cosine Similarity Score: {cosine_similarity * 100:.2f}%\n")
            file.write(f"BERTScore Similarity: {bert_similarity * 100:.2f}%\n")
            file.write(f"Extracted Key Information: {extracted_info}\n")
            file.write(f"Final NLP-based Score: {final_score * 100:.2f}%\n")

        print(f"Results saved in {output_file}")
    except Exception as e:
        print(f"Error occurred: {e}")

# Example usage
analyze_answer_sheet("/content/img.jpg", "/content/key_answer.txt")  # Uncomment and replace paths

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spellchecker import SpellChecker
import string

# Download necessary NLTK data
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Initialize SpellChecker
spell = SpellChecker()

# Function to clean and preprocess extracted text
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation (keep letters and spaces only)
    text = re.sub(f'[{string.punctuation}]', '', text)

    # Tokenize and remove stopwords
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]

    # Correct spelling mistakes
    corrected_text = [spell.correction(word) for word in filtered_text]

    return " ".join(corrected_text)

# Function to extract text from an image using EasyOCR
def extract_text_from_image(image_path):
    reader = easyocr.Reader(['en'])
    results = reader.readtext(image_path)
    extracted_text = " ".join([text for bbox, text, confidence in results])
    return preprocess_text(extracted_text)  # Preprocess the extracted text

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spellchecker import SpellChecker
import string

# Download necessary NLTK data
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Initialize SpellChecker
spell = SpellChecker()

# Function to clean and preprocess extracted text
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation (keep letters and spaces only)
    text = re.sub(f'[{string.punctuation}]', '', text)

    # Tokenize and remove stopwords
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]

    # Correct spelling mistakes
    corrected_text = [spell.correction(word) for word in filtered_text]

    return " ".join(corrected_text)

# Function to extract text from an image using EasyOCR
def extract_text_from_image(image_path):
    reader = easyocr.Reader(['en'])
    results = reader.readtext(image_path)
    extracted_text = " ".join([text for bbox, text, confidence in results])
    return preprocess_text(extracted_text)  # Preprocess the extracted text

!pip install easyocr
!pip install bert-score
!pip install sentence-transformers transformers
!pip install transformers requests
!pip install nltk

import cv2
import easyocr
import torch
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
from sentence_transformers import SentenceTransformer, util
from bert_score import score
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spellchecker import SpellChecker

# Download necessary NLTK data
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Download the missing 'punkt_tab' resource


# Load models for similarity analysis
similarity_model = SentenceTransformer("all-mpnet-base-v2")
bert_model_name = "microsoft/deberta-xlarge-mnli"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Initialize SpellChecker
spell = SpellChecker()

# Function to clean and preprocess extracted text
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove special characters and numbers (only keep letters)
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenize and remove stopwords
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]

    # Correct spelling mistakes
    corrected_text = [spell.correction(word) for word in filtered_text]

    return " ".join(corrected_text)

# Function to extract text from an image using EasyOCR
def extract_text_from_image(image_path):
    reader = easyocr.Reader(['en'])
    results = reader.readtext(image_path)
    extracted_text = " ".join([text for bbox, text, confidence in results])
    return preprocess_text(extracted_text)  # Preprocess the extracted text

# Function to compute Cosine Similarity
def get_similarity_score(answer, answer_key):
    embeddings1 = similarity_model.encode(answer, convert_to_tensor=True)
    embeddings2 = similarity_model.encode(answer_key, convert_to_tensor=True)
    return util.pytorch_cos_sim(embeddings1, embeddings2).item()

# Function to compute BERTScore
def get_bert_score(answer, answer_key):
    P, R, F1 = score([answer], [answer_key], lang="en", model_type="microsoft/deberta-xlarge-mnli")
    return F1.mean().item()

# Function to extract key information using question answering pipeline
def extract_info(text, question="What are the key points?"):
    return qa_pipeline(question=question, context=text)['answer']

# Function to read text from a file
def read_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read().strip()

# Function to analyze an answer sheet image
def analyze_answer_sheet(image_path, key_file, output_file="result.txt"):

import nltk
nltk.download('punkt')

!pip install pyspellchecker

!pip uninstall pytesseract
pip install pytesseract

!pip install bert-score

!pip install sentence-transformers transformers

!pip install transformers requests



def compute_similarity_score(answer_text, answer_key):
    # Encode both the extracted answer and the answer key
    embeddings1 = model.encode(answer_text, convert_to_tensor=True)
    embeddings2 = model.encode(answer_key, convert_to_tensor=True)

    # Compute cosine similarity
    similarity_score = util.cos_sim(embeddings1, embeddings2).item()
    return similarity_score
answer_text = '/content/Air_Pollution.txt'
answer_key = '/content/Student_ANS.txt'
compute_similarity_score(answer_text, answer_key)

def award_marks(similarity_score, max_marks):
    if similarity_score > 0.85:  # Very high similarity
        return max_marks
    elif similarity_score > 0.7:  # High similarity
        return int(0.8 * max_marks)
    elif similarity_score > 0.5:  # Moderate similarity
        return int(0.5 * max_marks)
    else:  # Low similarity
        return 0

# Example: Text extracted from the image and the answer key
extracted_text = "/content/Air_Pollution.txt"
answer_key = "/content/Student_ANS.txt"

# Compute similarity score
similarity_score = compute_similarity_score(extracted_text, answer_key)
print(f"Similarity Score: {similarity_score:.2f}")

# Award marks
max_marks = 10  # Assume full marks are 10
awarded_marks = award_marks(similarity_score, max_marks)
print(f"Marks Awarded: {awarded_marks}/{max_marks}")

award_marks(similarity_score, max_marks)

"""**LANGFLOW INTEGRATION**"""

import google.generativeai as genai
import PIL.Image

genai.configure(api_key="AIzaSyAwtrQ608X4k4fiedjPKmnOsVxhXdDLIWU")

model = genai.GenerativeModel("gemini-1.5-pro")
organ = PIL.Image.open("/content/img.jpg")
response = model.generate_content(["Analyze the given image and carefully extract the information.",organ])
print(response.text)